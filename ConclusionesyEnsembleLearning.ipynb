{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conclusiones.ipynb",
      "provenance": [],
      "mount_file_id": "19v0o5fBsle4rMvNf0-b84-6cfpWtEgvi",
      "authorship_tag": "ABX9TyOOH+5M3RCO2dl7xq1eFuhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfpIE16/SL_I_Tareas/blob/master/ConclusionesyEnsembleLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_mbhI9j04x3",
        "colab_type": "text"
      },
      "source": [
        "#Ensemble learning\n",
        "José Fernando Pérez Pérez\n",
        "\n",
        "*josefernando.perez@galileo.edu*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "En este notebook utilizaremos la técnica Ensemble learning para poder realizar una predicción final sobre el conjunto de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h4tsTjb0veq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "from scipy import stats\n",
        "import sklearn.metrics as mt\n",
        "import pandas as pd"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7nfjV4g0-ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = np.load(\"/content/drive/My Drive/test_data.npz\")\n",
        "x_test = test_data[\"x_test\"]\n",
        "y_test = test_data[\"y_test\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf6NLdHJ4ss3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(y_hat, y):\n",
        "  '''\n",
        "  get_metrics: obtiene un conjunto de metricas a partir del conjunto de datos\n",
        "  real y las predicciones hechas\n",
        "  Input:\n",
        "    y_hat - vector de predicciones\n",
        "    y     - vector de valores reales\n",
        "  Output:\n",
        "    metrics:\n",
        "      [0] - Accuracy\n",
        "      [1] - F1 Score\n",
        "      [2] - Precision Score\n",
        "      [3] - Recall Score\n",
        "  '''\n",
        "  metrics = [mt.accuracy_score(y, y_hat, normalize=True),\n",
        "             mt.f1_score(y, y_hat),\n",
        "             mt.precision_score(y, y_hat),\n",
        "             mt.recall_score(y, y_hat, average='weighted')]\n",
        "  return metrics"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGH_hbwk3CcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcion para realizar inferencias de Regresion Logistica\n",
        "def inference_log_reg(x, weights):\n",
        "  def sigmoid(l):\n",
        "    return (1 / (1 + np.exp(-l)))\n",
        "  x_f = np.insert(x, 0, 1, axis=1)\n",
        "  logits = np.matmul(x_f, weights)\n",
        "  y_hat = 1.0*(sigmoid(logits) > 0.5)\n",
        "\n",
        "  return y_hat"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_VTp8wf1slh",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función que se encargara de realizar las predicciones con base en los 3 modelos creados, se cargaran los modelos a partir de los archivos generados por cada modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O8bgeUz1r0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def titanicPrediction(x):\n",
        "  # Carga de modelos\n",
        "  # Cargar el modelo SVM\n",
        "  svm_model = joblib.load(\"/content/drive/My Drive/svm_final_3.pkl\")\n",
        "  # Cargamos los pesos para Regresion logistica\n",
        "  reg_log = np.load(\"/content/drive/My Drive/reg_log_weights.npz\")\n",
        "  weights = reg_log[\"W\"]\n",
        "  # Carga modelo Naive Bayes implementado con Scikit-learn\n",
        "  NB_model = joblib.load(\"/content/drive/My Drive/final_NB.pkl\")\n",
        "\n",
        "  # Predicciones\n",
        "  y_hat_svm = svm_model.predict(x)\n",
        "  y_hat_reglog = inference_log_reg(x, weights)\n",
        "  y_hat_nv = NB_model.predict(x[:, [2,9]]) # Utilizamos la combinacion optima\n",
        "\n",
        "  # Creamos una matriz con cada una de las predicciones\n",
        "  y_hat_matrix = np.column_stack((y_hat_svm, y_hat_reglog, y_hat_nv))\n",
        "\n",
        "  y_hat_final, _ = stats.mode(y_hat_matrix, axis = 1)\n",
        "\n",
        "  return y_hat_final, y_hat_matrix"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7icHjiRi4NKY",
        "colab_type": "text"
      },
      "source": [
        "## Predicción final\n",
        "El valor final es la moda de las 3 columnas, nos indica el valor que se repite más y dicho valor se toma como la predicción final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umxVO01I4YbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8c242026-aed8-434f-b0e8-6516749f1494"
      },
      "source": [
        "y_hat, y_hat_matrix = titanicPrediction(x_test)\n",
        "\n",
        "resultados = pd.DataFrame(y_hat_matrix,\n",
        "                           columns = [\"SVM\", \"Logistic_Regression\", \"Naive_Bayes\"])\n",
        "resultados"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVM</th>\n",
              "      <th>Logistic_Regression</th>\n",
              "      <th>Naive_Bayes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SVM  Logistic_Regression  Naive_Bayes\n",
              "0    0.0                  1.0          1.0\n",
              "1    0.0                  0.0          0.0\n",
              "2    1.0                  1.0          0.0\n",
              "3    0.0                  0.0          0.0\n",
              "4    0.0                  0.0          0.0\n",
              "..   ...                  ...          ...\n",
              "174  0.0                  0.0          0.0\n",
              "175  0.0                  0.0          0.0\n",
              "176  1.0                  1.0          0.0\n",
              "177  0.0                  0.0          0.0\n",
              "178  0.0                  0.0          0.0\n",
              "\n",
              "[179 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMXNYGhE-Lbg",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C5JOL_I4hUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dc85d2e-f492-4ab2-821b-3128292e5e6d"
      },
      "source": [
        "print(\"Accuracy : %s\"%round(get_metrics(y_hat, y_test)[0],2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eid7eBfh_qPA",
        "colab_type": "text"
      },
      "source": [
        "Se obtuvo una exactitud del **84%** sobre el conjunto de datos de prueba, cabe destacar que no se habían utilizado estos datos en ningún momento de la implementación del proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7X2_8uCB6xm",
        "colab_type": "text"
      },
      "source": [
        "# Pasos a seguir y K-folds\n",
        "\n",
        "- Investigar y practicar seguido el tema de *feature engineering*, dado que es un tema muy importante en la implementación de modelos predictivos.\n",
        "- La técnica **K-folds** permite tomar un subconjunto aleatorio de datos a partir del conjunto original, lo cual sería una ventaja para evitar valores de *sesgo* altos y nos permite tener mejores modelos por la aleatoriedad de los subconjuntos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt-rsyCjDfM7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusiones\n",
        "\n",
        "- Este proyecto me ha ayudado mucho personalmente en el sentido que he perdido un poco el miedo al momento de analizar los datos y con el complemento de las clases se que esta pasando en cada parte del proyecto. Los modelos dejaron de ser cajas negras, ahora ya entiendo un poco mejor su funcionamiento.\n",
        "- En mi caso la mayor dificultad fue el ajuste de los modelos, principalmente en *Logistic Regression*.\n",
        "\n",
        "## Lecciones aprendidas\n",
        "\n",
        "- *Feature engineering* es sumamente necesario para un científico de datos, tengo que aprender muchas tecnicas tanto de numpy, pandas y programación convencional para poder llevar a cabo este proceso de una forma mas fluida.\n",
        "- Las lecturas me han ayudado bastante a entender los modelos y como poder mejorar los parametros, sin embargo, creo que aún así debo leer más y complementar con prácticas.\n",
        "- De mi parte tengo que dar una mayor ventana de tiempo para la implementación del proyecto y no dejarlo para las ultimas semanas. En el curso siguiente tratare de avanzar mediante el flujo normal de la clase.\n",
        "\n",
        "## Recomendaciones\n",
        "\n",
        "- Para *Logistic Regression* me quedo pendiente utilizar *L1 Regularization* y consideró que habría sido de mucha utilidad ya que creo que mi conjunto de datos finales tenía algunas columnas que no aportaban nada al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkxOpdyVFMMI",
        "colab_type": "text"
      },
      "source": [
        "# Deployment\n",
        "Se utiliza la función titanicPrediction para realizar inferencia de conjuntos de datos particulares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO7YbI-wFLwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2bdde1b4-7931-42cf-ace1-da9a193394cc"
      },
      "source": [
        "random_data = np.array(([1,12.,0,1,1,1,0,1,0,85],\n",
        "                       [0,6.,1,0,0,1,0,1,0,14]))\n",
        "y_hat, y_hat_mat = titanicPrediction(random_data)\n",
        "y_hat_mat"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}